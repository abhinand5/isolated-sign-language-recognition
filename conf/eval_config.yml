general: 
  EXP_ID: exp5.2
  BASE_DIR: /notebooks/isolated-sign-language-recognition/
  BASE_DATA_DIR: /notebooks/data
  PREPROCESS_DATA: False
  TRAIN_MODEL: True
  DISTILL_TRAIN: True
  TRAIN_VALID_FINAL: False
  TRAIN_QAT_AWARE: False
  SEED: 555
  IS_INTERACTIVE: False
  VERBOSE: 2
  LOG_DIR: /notebooks/isolated-sign-language-recognition/logs/

data: 
  N_ROWS: 543
  N_DIMS: 3
  DIM_NAMES: ['x', 'y', 'z']
  NUM_CLASSES: 250
  INPUT_SIZE: 32

train:
  BATCH_ALL_SIGNS_N: 4
  BATCH_SIZE: 384
  N_EPOCHS: 100
  EARLY_STOPPING_PATIENCE: null
  N_EPOCHS_VALID_FIT: null
  LR_MAX: 1.0e-3
  N_WARMUP_EPOCHS: 0
  WD_RATIO: 0.05
  MASK_VAL: 4237
  # Here K_FOLDS = 1 means single split
  K_FOLDS: 1
  FOLDS_TO_TRAIN: [0]
  # TO be set if K_FOLDS = 1, by default this value is 0.1
  VAL_RATIO: 0.1
  # To be set if K_FOLDS = 1
  TRAIN_ON_ALL_DATA: False
  LOAD_MODELS: False
  LOAD_MODELS_MAP: null

# If Knowledge Distillation is true then,
# the below config is used for teacher model
model:
  # Dense layer units for landmarks
  LIPS_UNITS: 256
  HANDS_UNITS: 256
  POSE_UNITS: 256
  # Num attention heads
  MHA_HEADS: 4
  # final embedding and transformer embedding size
  UNITS: 512

  # Transformer
  NUM_BLOCKS: 2
  MLP_RATIO: 2
  ADD_LAYER_NORM: True
  LAYER_NORM_EPS: 1.0e-6
  # Dropout
  EMBEDDING_DROPOUT: 0.00
  MLP_DROPOUT_RATIO: 0.30
  CLASSIFIER_DROPOUT_RATIO: 0.10
  # Activations
  ACTIVATION_FN: gelu
  # Optimizer
  INIT_LR: 1.0e-03
  WT_DECAY: 1.0e-05
  LABEL_SMOOTHING: 0.3
  CB_MONITOR: val_acc
  CB_MONITOR_MODE: max
  MODEL_DIR: /notebooks/isolated-sign-language-recognition/models